{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# COMP90089 Biomedical Time-Series tutorial\n",
    "\n",
    "This tutorial analyzes an ECG dataset.\n",
    "The ECG dataset comprises long-term Electrocardiogram (ECG) recordings of human subjects diagnosed with atrial fibrillation. The dataset captures two-channel distinct ECG signals, both sampled at a rate of 250 Hz and annotated with four different classes: [AFIB (atrial fibrillation), (AFL (atrial flutter), (J (AV junctional rhythm), and (N (used to indicate all other rhythms)](https://physionet.org/content/afdb/1.0.0/old/).\n",
    " \n",
    "The data has been divided into training, validation, and test. Each sample contains a 2-channel ECG lasting 10s, i.e., 2500 points. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "vgiu55Gs1HuD",
    "outputId": "b5d861a9-6921-41eb-9c02-410424531e52",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, ensure you have the necessary packages installed in your Google Colab environment.\n",
    "# You can install them by running:\n",
    "!pip install torch pandas numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R51oUGMS2xtL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "y1szLD4f3waM",
    "outputId": "fc6f4afc-f703-42ec-e910-3e6da5078667"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load datasets from .pkl files\n",
    "x_train = pd.read_pickle('https://github.com/melbourne-cdth/comp90089_time_series_tutorial/raw/refs/heads/main/ECGless/x_train1.pkl')\n",
    "x_val = pd.read_pickle('https://github.com/melbourne-cdth/comp90089_time_series_tutorial/raw/refs/heads/main/ECGless/x_val1.pkl')\n",
    "x_test = pd.read_pickle(\"https://github.com/melbourne-cdth/comp90089_time_series_tutorial/raw/refs/heads/main/ECGless/x_test1.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle('https://github.com/melbourne-cdth/comp90089_time_series_tutorial/raw/refs/heads/main/ECGless/y_train1.pkl')\n",
    "y_val = pd.read_pickle('https://github.com/melbourne-cdth/comp90089_time_series_tutorial/raw/refs/heads/main/ECGless/y_val1.pkl')\n",
    "y_test = pd.read_pickle('https://github.com/melbourne-cdth/comp90089_time_series_tutorial/raw/refs/heads/main/ECGless/y_test1.pkl')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Plot a feature from the datasets\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot a feature from x_train\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_train[20, 0], label='Train Feature 0')\n",
    "plt.title('Training Data - ECG Channel 1')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('ECG')\n",
    "plt.legend()\n",
    "\n",
    "# Plot a feature from x_val\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_val[10, 0], label='Validation Feature 0')\n",
    "plt.title('Validation Data - ECG Channel 1')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('ECG')\n",
    "plt.legend()\n",
    "\n",
    "# Plot a feature from x_test\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x_test[10, 0], label='Test Feature 0')\n",
    "plt.title('Test Data - ECG Channel 1')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('ECG')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfhHhTE7f0Qy"
   },
   "outputs": [],
   "source": [
    "chunk_size = 500  # Chunking a sequence of 2500 points into 5 segments, each containing 500 points.\n",
    "\n",
    "class TimeSeriesDataset(Dataset): # chunk each 2500 into segments for LSTM to learn temporal dynamics\n",
    "    def __init__(self, data, labels, chunk_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_chunks = data.shape[2] // chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        chunks = [sequence[:, i * self.chunk_size:(i + 1) * self.chunk_size] for i in range(self.num_chunks)]\n",
    "        chunks = np.stack(chunks, axis=0)  # Shape: (5, 2, 500)\n",
    "        return torch.tensor(chunks).float(), label\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 256\n",
    "dataset = TimeSeriesDataset(x_train, y_train, chunk_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataset = TimeSeriesDataset(x_val, y_val, chunk_size)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataset = TimeSeriesDataset(x_test, y_test, chunk_size)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRxZuaLc33I9"
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_chunks, chunk_size, num_features)\n",
    "        batch_size, num_chunks, chunk_size, num_features = x.size()\n",
    "        x = x.view(batch_size * num_chunks, chunk_size, num_features)  # Reshape for LSTM\n",
    "\n",
    "        # Get LSTM outputs\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "\n",
    "        # hn: (num_layers, batch_size * num_chunks, hidden_size)\n",
    "        # Reshape hn to get hidden states for each chunk\n",
    "        hn = hn[-1].view(batch_size, num_chunks, -1)  # Shape: (batch_size, num_chunks, hidden_size)\n",
    "\n",
    "        # Aggregate across chunks for final classification: for example, take the mean\n",
    "        # Here you might choose another aggregation strategy if needed\n",
    "        aggregated_hidden_state = torch.mean(hn, dim=1)  # Example aggregation\n",
    "\n",
    "        # Final output using the aggregated hidden states\n",
    "        out = self.fc(aggregated_hidden_state)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Return both the final output and the full hidden states\n",
    "        return out, hn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, nhead, dim_feedforward, output_size, problem_type):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.problem_type = problem_type\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if problem_type == 'classification':\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is (batch_size, seq_length, input_size) and needs to be transposed\n",
    "        x = x.permute(1, 0, 2)  # Transpose to (seq_length, batch_size, input_size)\n",
    "        out = self.transformer_encoder(x)\n",
    "        out = self.fc(out[-1])  # Use the last output for classification or regression\n",
    "        if self.problem_type == 'classification':\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = chunk_size\n",
    "problem_type = 'classification'\n",
    "hidden_size = 64\n",
    "num_layers = 4\n",
    "output_size = 4 # how many classes\n",
    "\n",
    "# Instantiate the model\n",
    "# model = LSTMModel(input_size, hidden_size, num_layers, output_size, problem_type)\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# # # switching to Transformer model\n",
    "# input_size = X_train.shape[-1]\n",
    "# num_layers = 2\n",
    "# nhead = 2              # Number of attention heads\n",
    "# dim_feedforward = 64\n",
    "# output_size = 4\n",
    "# problem_type = 'classification'\n",
    "\n",
    "# # Create an instance of the TransformerModel\n",
    "# model = TransformerModel(input_size, num_layers, nhead, dim_feedforward, output_size, problem_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yKB6yvk2G_g1",
    "outputId": "0624d89d-e764-47dd-fcdd-a8c67bf4cecc"
   },
   "outputs": [],
   "source": [
    "if problem_type == 'regression':\n",
    "    criterion = nn.MSELoss()\n",
    "elif problem_type == 'classification':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 3e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    betas=(beta1, beta2)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "# Training the model with validation and early stopping\n",
    "num_epochs = 150\n",
    "patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        outputs, _ = model(x_batch)\n",
    "        if problem_type == 'classification':\n",
    "            y_batch = y_batch.long()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            val_outputs, _ = model(x_val)\n",
    "            if problem_type == 'classification':\n",
    "                y_val = y_val.long()\n",
    "            val_loss += criterion(val_outputs, y_val).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'Best model saved at epoch {epoch+1} with validation loss: {val_loss:.4f}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FtKEeJc39Bm",
    "outputId": "555cd815-19bd-4206-cb6b-4ee2b263da4c"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_outputs_list = []\n",
    "y_test_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_test_chunk, y_test_chunk in test_loader:\n",
    "        test_outputs, hidden_states = model(x_test_chunk)\n",
    "        test_outputs_list.append(test_outputs)\n",
    "        y_test_list.append(y_test_chunk)\n",
    "\n",
    "# Concatenate all outputs\n",
    "y_test_pred = torch.cat(test_outputs_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "if problem_type == 'classification':\n",
    "  y_test_pred_class = torch.argmax(y_test_pred, dim=1)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y_test_pred = model(X_test)\n",
    "#     y_test_pred = y_test_pred.squeeze()\n",
    "#     if problem_type == 'classification':\n",
    "#         y_test_pred_class = torch.argmax(y_test_pred, dim=1)\n",
    "\n",
    "# Calculate metrics\n",
    "if problem_type == 'regression':\n",
    "    y_test_actual = y_test.numpy()\n",
    "    predicted = y_test_pred.numpy()\n",
    "    mae = mean_absolute_error(y_test_actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual, predicted))\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "\n",
    "    # Plot predictions vs actual values\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(predicted, label='Predicted', alpha=0.7)\n",
    "    plt.plot(y_test_actual, label='Actual', alpha=0.7)\n",
    "    plt.title('Predicted vs Actual Values')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "elif problem_type == 'classification':\n",
    "    y_test_class = y_test.int().numpy()\n",
    "    accuracy = accuracy_score(y_test_class, y_test_pred_class)\n",
    "    precision = precision_score(y_test_class, y_test_pred_class, average='macro')\n",
    "    recall = recall_score(y_test_class, y_test_pred_class, average='macro')\n",
    "    f1 = f1_score(y_test_class, y_test_pred_class, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test_class, y_test_pred_class)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q5WI9wHna4pK",
    "outputId": "25908c47-8c33-4266-ca1f-9f4b16ddbb9f"
   },
   "outputs": [],
   "source": [
    "# Select the hidden states for the first (and only) example\n",
    "hn_example = hidden_states[0].detach().numpy()  # Shape: (num_chunks, hidden_size)\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12))  # Create a figure with 2 rows and 1 column\n",
    "\n",
    "# Top plot: Heatmap of Hidden States\n",
    "axes[0].imshow(hn_example.T, aspect='auto', cmap='viridis')\n",
    "axes[0].set_title('Heatmap of Hidden States')\n",
    "axes[0].set_ylabel('Hidden Unit')\n",
    "axes[0].set_xlabel('Chunk')\n",
    "cb = plt.colorbar(axes[0].images[0], ax=axes[0], orientation='vertical')\n",
    "cb.set_label('Activation')\n",
    "\n",
    "# Bottom plot: ECG Time Series\n",
    "print(x_test_chunk[0, :, 0].shape)\n",
    "axes[1].plot(x_test_chunk[0, :, 0].reshape(-1))\n",
    "axes[1].set_title('ECG Time Series')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Value')\n",
    "\n",
    "for x in range(500, 2500, 500):\n",
    "    axes[1].axvline(x=x, color='red', linestyle='--', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "fMmLk08tHttI",
    "outputId": "2d8711c8-f14e-4bfb-f4f1-c6af105d724a"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to extract features from the LSTM\n",
    "def extract_features(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(next(model.parameters()).device)\n",
    "            outputs, _ = model(inputs)\n",
    "            feature = outputs.cpu().numpy()\n",
    "            features.append(feature)\n",
    "            labels.append(targets.numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "# Apply t-SNE to the features\n",
    "def plot_tsne(features, labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Class Label')\n",
    "    plt.title('t-SNE Visualization of LSTM Features')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "features, labels = extract_features(model, train_loader)\n",
    "plot_tsne(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
